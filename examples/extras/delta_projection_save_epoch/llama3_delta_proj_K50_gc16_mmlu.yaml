model_name_or_path: meta-llama/Meta-Llama-3-8B
adapter_name_or_path: /home/ubuntu/date/mq_tst/new_era_proj/saves/llama3_delta_proj_K50_gc16

quantization_bit: 4
template: llama3
finetuning_type: delta

lora_target: all
lora_rank: 64
lora_alpha: 64

### dataset
task: mmlu
split: test
template: fewshot
lang: en
n_shot: 5

### output
save_dir: /home/ubuntu/date/mq_tst/new_era_proj/saves/llama3_delta_proj_K50_gc16/evals

### eval
batch_size: 2

use_delta: true
badam_mode: layer
badam_switch_mode: ascending
badam_switch_interval: 5
badam_verbose: 1